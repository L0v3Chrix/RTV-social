# Experimentation OS — v1

**Purpose:** A controlled experimentation operating system for an agency-run, semi-autonomous social media automation platform.

This OS lets you:
- run **high-volume creative experiments** safely
- iterate fast without blowing up accounts
- standardize “what we test, how we ship, how we decide”
- support **A/B tests** and **bandit-style rotation** (post-MVP) while staying inside compliance guardrails

**Non-goals (MVP):**
- full causal attribution or conversion lift measurement
- automated manipulation / inauthentic engagement patterns

---

## 1) Core idea: experiments are first-class objects

Everything the system outputs (plans, posts, replies, assets) can be tied back to an **Experiment**.

### What an Experiment is (in this system)
A time-bounded set of **variants** designed to learn one thing.

Examples:
- Which **hook style** gets higher 3-second retention on Reels?
- Which **caption structure** drives more meaningful comments?
- Which **thumbnail cover** increases Shorts view-to-sub ratio?

---

## 2) MVP Experiment Types (what we support day-one)

### 2.1 Micro-Experiment (single variable)
- **Unit:** one post per variant (or small set of posts)
- **Goal:** learn quickly
- **Risk:** low
- **Use:** hook swaps, CTA swaps, format swaps

### 2.2 Batch Experiment (weekly)
- **Unit:** 5–20 posts across a week
- **Goal:** isolate platform behavior patterns and audience resonance
- **Risk:** medium (volume)
- **Use:** blueprint family comparisons

### 2.3 Engagement Experiment (assist-mode)
- **Unit:** reply patterns (suggested vs published)
- **Goal:** learn “best reply templates” without automation risk
- **Risk:** low (human-in-loop)

### 2.4 Lane Experiment (API vs Browser vs Hybrid)
- **Unit:** publishing execution strategy
- **Goal:** reliability + operational latency
- **Risk:** medium (publishing failures)

---

## 3) The Experiment Lifecycle

### Stage A — Intake (Experiment Brief)
Every experiment starts with a brief that can be created in the UI (or CLI).

**ExperimentBrief schema:**
- **name**
- **client_id / platform(s)**
- **surface** (Feed/Reels/Stories/Shorts/etc.)
- **hypothesis** (1 sentence)
- **variable_under_test** (only one in MVP)
- **control_variant**
- **test_variants[]**
- **success_proxies** (platform-appropriate)
- **guardrails** (account health + negative signals)
- **exposure_plan** (schedule + pacing)
- **run_window** (start/end)
- **stop_conditions**
- **approval_level** (auto / review / manual)

### Stage B — Variant Generation
Variants are generated by **Creator agents** under strict constraints:
- keep all non-tested variables constant
- obey BrandKit + CompliancePolicy
- produce asset + caption + metadata

### Stage C — Preflight (trust + safety checks)
Must pass before publishing:
- **PolicyGuard** (allowed actions for platform)
- **BudgetGuard** (cost/latency)
- **FormatGuard** (size, ratio, duration)
- **UniquenessGuard** (avoid near-duplicates)
- **RiskScan** (regulated claims/disallowed content)

### Stage D — Launch (publishing)
- schedule variants in the calendar with deliberate spacing
- enforce **pacing** to avoid spam signals
- publish via API where possible
- browser lane requires human-in-loop unless explicitly permitted

### Stage E — During-Run Monitoring
Real-time monitoring watches:
- publish success/failure
- platform warnings
- hard guardrail thresholds

### Stage F — Postflight Verification
- confirm post is live and visible
- store proof (URL + optional screenshot)

### Stage G — Outcome Review
- rank variants by **proxy outcomes**
- log learnings to Memory
- decide: ship winner, iterate, or abandon

---

## 4) Metrics in MVP: proxy signals + operational signals

Because we’re not doing full performance attribution in MVP, we use:

### 4.1 “Algorithm proxy” signals (per platform)
- **Retention proxies:** view duration, completion, replays (where available)
- **Conversation proxies:** comment depth, reply chains, meaningful replies
- **Distribution proxies:** shares, saves, sends

### 4.2 Operational signals (system learning)
- publish reliability rate
- tool error rate per lane
- time-to-draft, time-to-publish
- human approval friction (reject rate, revision count)

---

## 5) Guardrails: experiment quality + platform safety

### 5.1 Guardrail metrics (account survival)
Guardrails trigger auto-pause and escalation.

**Examples:**
- platform warning events
- login challenges
- sudden spike in publish failures
- negative feedback signals (hides, reports) where available

### 5.2 Experiment-quality checks (trustworthiness)
Classic online experimentation programs emphasize quality checks before trusting results. We apply analogous checks:

- **Exposure Ratio Mismatch (ERM):**
  - if variants did not get comparable distribution windows
  - if one variant posts at a drastically different time/slot
  - if one variant had a publish error or platform throttling

- **Instrumentation sanity:**
  - verify metrics collection is working
  - verify content metadata stored correctly

When ERM occurs, experiment is marked **invalid** and outcomes are “informational only.”

---

## 6) Decision rules (MVP)

MVP uses simple decision rules with clear stop conditions:

### 6.1 When to stop early
Stop experiment early if:
- any guardrail breach triggers
- publish failures exceed threshold
- ERM invalidates the test

### 6.2 When to declare a winner
Declare a winner when:
- at least N comparable posts per variant
- winner leads on ≥2 proxy metrics without guardrail issues
- review agent confirms no confounding variable

### 6.3 Human approval points
- regulated industries
- big offer/claims changes
- browser lane runs
- any escalation event

---

## 7) Experiment scheduling: “layers” to avoid interference

When multiple experiments run concurrently, they can contaminate results.

We implement **Experiment Layers**:
- a layer is a namespace where experiments are mutually exclusive
- a client can run only one “hook experiment” in the same surface at a time

**Layer examples:**
- IG_Reels_Hook
- IG_Feed_Carousel
- TikTok_Hook
- YouTube_Shorts_Cover

---

## 8) Post-MVP: Bandit engine for content allocation

Once metrics are instrumented (V2), we add a “Bandit Scheduler”:
- continues exploring new variants while exploiting winners
- auto-allocates more posting slots to better-performing variants

**Key design:**
- bandit decisions are constrained by CompliancePolicy + pacing rules
- bandit operates per-client and per-platform surface

---

## 9) Core data objects (minimum viable)

### 9.1 Experiment
- experiment_id
- client_id
- platforms + surfaces
- hypothesis + variable_under_test
- status (draft/running/paused/completed/invalid)
- start/end
- guardrails + stop conditions

### 9.2 Variant
- variant_id
- experiment_id
- control/test flag
- asset_refs (video/image)
- copy_refs
- metadata (hashtags, title, cover)

### 9.3 ExposureRecord
- post_id
- variant_id
- publish_timestamp
- surface
- lane used

### 9.4 OutcomeSnapshot
- post_id
- captured_at
- proxy_metrics blob

### 9.5 ExperimentReport
- summary
- validity checks (ERM, publish reliability)
- winner + confidence notes
- next iteration suggestion

---

## 10) MVP UX: how operators use it (2–3 check-ins/day)

### Morning check-in (10–15 minutes)
- approve today’s experiment queue
- confirm no guardrail alerts
- ensure layers aren’t colliding

### Midday check-in (10–15 minutes)
- verify posts published
- handle escalations
- enqueue engagement replies

### End-of-day (5–10 minutes)
- quick outcome snapshot
- tag any experiments for follow-up

---

## 11) Integration with Functional Learning (system-level)

Experimentation OS ties into Adaptive Learning by writing:
- per-tool failure patterns
- per-agent rewrite patterns
- per-blueprint friction + revision counts
- lane reliability metrics

This upgrades the system’s internal performance without needing full campaign attribution.

---

## 12) MVP build checklist

- [ ] ExperimentBrief UI + CLI
- [ ] Variant generator (single-variable enforcement)
- [ ] Preflight gates wired to Policy/Budget/Format
- [ ] Calendar scheduling with layers
- [ ] Publish + verification
- [ ] Outcome snapshots + ERM validity check
- [ ] ExperimentReport writer + memory write
- [ ] Guardrail auto-pause + escalation

---

## 13) Next document
**Observability & Check-In Dashboard Spec — v1**

