# F) Agent Prompt Ops Manual — v1

**Project:** Raize The Vibe — Autonomous Social Media Automation Platform (Agency-Operated)

**Audience:** Engineering + Ops + Creative teams using **vibecoding** (CLI agents + LLM copilots + MCP tool gateways).

**Purpose:** Treat agent prompts like production code.

- Prevent prompt drift
- Make outputs deterministic enough to operate
- Enable fast iteration without regressions
- Keep tool use safe (no wrong-tenant side effects)
- Make every prompt change measurable (evals + budgets + rollout)

This manual defines:
- Prompt architecture + naming
- How prompts are written, tested, versioned, deployed
- How prompts interact with tools + external memory (RLM)
- How we avoid context bloat (Docker MCP gateway approach)
- How we run regressions (golden sets + sampling)

---

## 0) Non‑negotiables

1) **Prompts are code**
- Stored in repo
- Versioned
- Reviewed
- Tested
- Released with changelog

2) **Side effects are never “implied”**
- Prompt text alone cannot authorize publish/DM/like actions
- Side effects must pass: RBAC + Policy + Feature flags + Approval gates

3) **Every prompt has a contract**
- Inputs
- Outputs (schema)
- Allowed tools
- Budgets
- Failure modes

4) **Everything is observable**
- Prompt execution emits:
  - episode trace
  - cost + latency
  - tool calls summary
  - eval score (when applicable)

---

## 1) Prompt taxonomy (what kinds of prompts exist)

### 1.1 System prompts
**Goal:** Immutable behavior constraints.

Examples:
- Tenant isolation rules
- Safety rules for engagement
- “No secrets in output” rules

System prompts are:
- minimal
- high-leverage
- extremely stable

### 1.2 Agent role prompts
**Goal:** Define a specialized worker with a clear job.

Examples:
- Planner Agent
- Creator Agent
- Publisher Agent
- Engagement Agent
- Verifier Agent
- Doc/Journaling Agent
- Orchestrator Agent

### 1.3 Task prompts
**Goal:** A one-off execution request with specific inputs.

Examples:
- “Create a weekly plan for Tenant X using Blueprint set Y”
- “Generate 5 reels from Plan node N”

Task prompts are generated by the runner using:
- tenant config snapshots
- retrieval summaries
- plan/asset graph references

### 1.4 Tool description prompts
**Goal:** Teach agents how to use tools correctly.

This includes:
- tool name + purpose
- allowed/forbidden uses
- input schema
- examples
- what to do on errors

### 1.5 Eval prompts
**Goal:** deterministic evaluation / grading prompts.

We use:
- rubric-based scoring
- schema-based checks
- policy compliance checks

---

## 2) Repository layout (required)

```
/agents/
  /system/
    safety.md
    tenant-isolation.md
    side-effects.md
  /roles/
    planner.md
    creator.md
    publisher.md
    engager.md
    verifier.md
    doc-agent.md
    orchestrator.md
  /tasks/
    plan.generate.md
    assets.generate.md
    schedule.build.md
    publish.execute.md
    engage.inbox-triage.md
  /tools/
    docker-mcp-gateway.md
    browser-runner.md
    platform-api.meta.md
    platform-api.linkedin.md
    ghlsync.md
  /evals/
    rubric.content-quality.md
    rubric.brand-voice.md
    rubric.policy-compliance.md

/evals/
  golden/
    planner.jsonl
    creator.jsonl
    engager.jsonl
  sampling/
    recent-production-traces.jsonl
  scripts/
    run_evals.ts
    score.ts

/docs/
  prompt-ops/
    PROMPT_CHANGELOG.md
    PROMPT_RELEASES.md
    PROMPT_STYLE_GUIDE.md
```

---

## 3) Prompt object standard (the contract)

Every prompt file begins with a **frontmatter contract**.

```yaml
id: planner_v1
name: Planner Agent
type: role
version: 1.0.0
owner: ops
risk_tier: B
allowed_tools:
  - retrieval.summary
  - plan.graph.write
  - budget.check
output_schema: schemas/plan_node.json
budgets:
  max_tokens: 1800
  max_tool_calls: 12
  max_runtime_seconds: 90
stop_conditions:
  - "if policy denies"
  - "if missing required brand kit fields"
notes:
  - "never propose medical/financial guarantees"
```

Then the prompt body.

**Rule:** If it lacks a contract header, it does not ship.

---

## 4) Prompt writing guidelines (house style)

### 4.1 Context engineering: keep it tight
We follow a “tight context” approach:
- include only what is needed for the current step
- prefer summaries over raw dumps
- retrieve context just-in-time

### 4.2 Use explicit sections
Use consistent headings:
- Objective
- Inputs
- Constraints
- Tooling
- Output Format
- Self-check
- Escalation

### 4.3 Prefer structured outputs
Whenever output is used by another agent/tool:
- return JSON following schema
- validate with Zod/Pydantic equivalent

### 4.4 Force evidence + references for any claim
For platform behavior:
- require “source-of-truth” citations stored in docs
- otherwise label as hypothesis

### 4.5 Avoid multi-step ambiguity
If a task includes multiple phases, the prompt must specify:
- phase order
- artifacts created per phase
- where state is written (plan graph / asset graph)

### 4.6 Built-in self-check
Every agent prompt ends with a short checklist:
- brand compliance
- policy compliance
- platform constraints
- output schema validation

---

## 5) Tool use policy inside prompts

### 5.1 Tool minimization principle (Docker MCP gateway style)
Agents should not load every tool description into context.

Instead:
1) Call **Tool Discovery** (gateway search)
2) Load only the 1–3 tools needed
3) Execute tools
4) Write large tool results to external artifacts
5) Summarize only the minimal conclusions back into context

### 5.2 Tool call discipline
For every tool call, the agent must:
- state intent (“why this tool”)
- include tenant scope
- include idempotency key if side effect
- handle failure states

### 5.3 No direct secret handling
Prompts must never:
- request raw API keys
- print tokens
- store credentials

They may reference:
- `CredentialRef` IDs
- “mint token” endpoints

---

## 6) Standard agent roles (required prompts)

### 6.1 Orchestrator Agent
**Purpose:** Convert objectives into small PR slices / job slices.

Responsibilities:
- break work into atomic steps
- assign tasks to subagents
- enforce budgets + stop conditions
- ensure docs + tests are appended

Outputs:
- `WorkPlan` JSON: list of slices, acceptance criteria, owners

### 6.2 Planner Agent
**Purpose:** Produce strategy plans that map to blueprints.

Inputs:
- BrandKit
- KB summaries
- enabled blueprint set
- cadence targets

Outputs:
- `PlanGraph` nodes with rationale + constraints

### 6.3 Creator Agent
**Purpose:** Turn plan nodes into platform-specific assets.

Outputs:
- `AssetGraph` entries:
  - copy variants
  - creative briefs
  - image prompts
  - video prompts (silent-first)
  - HeyGen script + shot plan

### 6.4 Publisher Agent
**Purpose:** Build schedule + prepare publish payloads.

Rules:
- never publishes without approval gates
- produces “ready-to-publish” objects + verification steps

### 6.5 Engagement Agent
**Purpose:** Triage inbox (comments/DMs) + respond safely.

Rules:
- uses policy binder and escalation rules
- classifies messages and suggests responses
- executes only if policy allows

### 6.6 Verifier Agent
**Purpose:** Prove correctness.

Checks:
- schema validation
- brand voice rubric
- policy compliance rubric
- platform format requirements
- side-effect preflight verification

### 6.7 Doc Agent
**Purpose:** Append documentation.

Outputs:
- ADR draft (if decision)
- journal entry
- runbook updates if new alert/failure class

---

## 7) Prompt versioning & lifecycle

### 7.1 Semantic versioning for prompts
- **MAJOR**: changes output schema or behavior contract
- **MINOR**: new capabilities that are backward compatible
- **PATCH**: bug fixes / clarity improvements

### 7.2 Prompt release channels
- `dev` (local)
- `staging` (sandbox accounts)
- `prod` (real clients)

### 7.3 Rollout requirements
For any prompt change affecting Tier C surfaces:
- run golden eval suite
- run 20–100 sample cases
- run sandbox publish/engage dry-run
- canary 1–2 tenants

---

## 8) Prompt evaluation system (regression & improvement)

### 8.1 Two test sets
1) **Golden set** (small, curated, deterministic)
- gating suite before merge

2) **Sampling set** (random / production traces)
- catches novel failures

### 8.2 What we score
- Brand voice adherence
- Platform format correctness
- Policy compliance
- Action safety (no side effects suggested when disallowed)
- Cost/latency budget adherence

### 8.3 Evaluation methods
- schema validators
- rubric graders
- diff-based regression checks
- “must include / must not include” checks

### 8.4 Evals in CI
Every PR that changes prompts must:
- run `evals:golden`
- pass threshold gates
- attach report artifact

---

## 9) Prompt change workflow (vibecoding standard)

### 9.1 The Prompt Change PR template (required)
Include:
- what changed
- why
- expected outcome
- risk tier impacted
- eval results (before/after)
- rollback plan (revert prompt version)

### 9.2 The “prompt slice” rule
Each PR changes:
- one role OR
- one task prompt OR
- one rubric

Avoid massive prompt sweeps.

### 9.3 Documentation updates
Every change appends to:
- `/docs/prompt-ops/PROMPT_CHANGELOG.md`
- `/docs/journal/YYYY-MM-DD.md`

---

## 10) Safety patterns for agents (high-leverage constraints)

### 10.1 Tenant safety clause (must appear in system)
- “Never reference or use another tenant’s data.”
- “All actions must include tenant_id.”

### 10.2 Side-effect clause
- “Never perform side effects without policy allow + approval token.”

### 10.3 Escalation clause
If any of these appear, escalate:
- pricing negotiations
- refunds/complaints
- legal threats
- regulated claims
- platform bans/strikes

---

## 11) Prompt templates (copy/paste)

### 11.1 Role prompt template

```
# {Agent Name}

## Objective

## Inputs

## Constraints

## Tools

## Output Format

## Self-check

## Escalation
```

### 11.2 Task prompt template

```
# Task: {task_name}

## Goal

## Required context

## Steps

## Output (schema)

## Verification

## Stop conditions
```

### 11.3 Rubric template

```
# Rubric: {name}

## Dimensions
- D1: ... (0–5)
- D2: ... (0–5)

## Fail conditions

## Notes
```

---

## 12) Post‑MVP extensibility patterns

- **Client logins**: split prompt policy for client vs operator actions
- **Per-client prompt overrides**: allow limited overrides only via structured fields, not free-form prompt edits
- **Prompt marketplace**: enable blueprint-prompt packs with validation and linting
- **Self-improving prompts**: add “prompt proposal agent” that suggests changes but cannot deploy without eval + approval

---

## 13) Definition of Done (Prompt Ops)

A prompt is “production-ready” when:
- contract header present
- output schema validated
- golden eval suite passes
- sampling eval passes threshold
- budgets defined and respected
- tool list minimized and correct
- docs appended
- rollback path defined

---

## 14) Next document

**G) Multi‑Tenant Isolation & Data Boundary Spec — v1**

